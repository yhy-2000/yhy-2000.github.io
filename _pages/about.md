---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>
<span style="font-style: italic; color: #6a994e; font-size: 16px;">Life is short, just have fun.</span> ğŸ¤ª



# ğŸ’ƒğŸ» About Me

- ğŸ“ I am a **Ph.D. candidate** at the [Gaoling School of Artificial Intelligence](https://ai.ruc.edu.cn/), [Renmin University of China](https://www.ruc.edu.cn/), supervised by Prof. [Zhicheng Dou](http://playbigdata.ruc.edu.cn/dou) and Prof. [Ji-Rong Wen](https://scholar.google.com/citations?user=tbxCHJgAAAAJ).  
- ğŸ”¬ Currently, I am a **research intern** at **Beijing Academy of Artificial Intelligence (BAAI)**, mentored by [Zheng Liu](https://zhengliu101.github.io/). I sincerely appreciate their meticulous and insightful guidance~ 
- ğŸ“ I received my bachelor's degree from **Nankai University** in 2022.  
-  **Research Interests**: <span style="color:#c1121f">Retrieval-augmented generation</span>, <span style="color:#c1121f">Multi-modal retrieval</span>, <span style="color:#c1121f">Long video understanding</span>  

---

# ğŸ“¢ News

- **[2025.09]**: [**MomentSeeker**] was accepted at **NIPS 2025** ğŸ‰ğŸ‰ Thanks to all co-authors!!

- **[2025.06]**: Starting this September, Iâ€™ll be a visiting student at ğŸ‡®ğŸ‡¹**University of Trento**, supervised by [Prof. Dr. Nicu Sebe](https://disi.unitn.it/~sebe/). Looking forward to new collaborations and challenges! ğŸš€

- **[2025.06]**: Released [**VideoDeepResearch**](https://arxiv.org/pdf/2506.10821), a novel framework combining a text-only LLM and a multi-modal toolkit to beat SOTA MLLMs.  ğŸ”— [Code on GitHub](https://github.com/yhy-2000/VideoDeepResearch)

- **[2025.03]**: Released [**MemVid**](https://arxiv.org/abs/2503.09149), a memory-enhanced RAG framework for long video understanding.

- **[2025.03]**: Paper [**OmniGen**](https://arxiv.org/abs/2409.11340) accepted at **CVPR 2025** ğŸ‰

- **[2025.02]**: Released [**MomentSeeker**](https://arxiv.org/abs/2502.12558), a task-oriented benchmark for long-video moment retrieval.

- **[2024.11]**: [**FineRAG**](https://aclanthology.org/2025.coling-main.741/) accepted at **COLING 2024** ğŸ¥³

- **[2023.08]**: [**VILE**](https://dl.acm.org/doi/10.1145/3583780.3615107) accepted at **CIKM 2023** ğŸ¥³

---

# ğŸ“ Education

### - Renmin University of China  (2022.09 â€“ 2027.06 (Expected)) 
- **Ph.D. in Artificial Intelligence**, Supervisor: Prof. Zhicheng Dou, Prof. Ji-Rong Wen  
- GPA: **3.87/4.0**, Key Courses: Intelligent Information Retrieval (A), Machine Learning (A)

###  - Nankai University  (2018.09 â€“ 2022.06) 
- **B.S. in Computer Science**, GPA: **91.64/100**, Rank: **2/116**
- CET-6: 538, CET-4: 585

---

# ğŸ“š Publications


### - **VideoDeepResearch: Long Video Understanding With Agentic Tool Using** | [Paper]([https://arxiv.org/abs/2503.09149](https://arxiv.org/pdf/2506.10821)) | [GitHub](https://github.com/yhy-2000/VideoDeepResearch) | *Preprint*

**H Yuan**, Z Liu, J Zhou, H Qian, JR Wen, Z Dou

> Proposes an agentic framework for long video understanding that leverages a text-only large reasoning model combined with a modular multi-modal toolkit. Achieves superior performance over state-of-the-art open-source and proprietary MLLMs, including GPT-4o, Qwen2.5-VL, and others, across a wide range of long video understanding benchmarks.


### - **MemVid: Memory-enhanced Retrieval Augmentation for Long Video Understanding**  | [Paper](https://arxiv.org/abs/2503.09149) | *Preprint*
**H Yuan**, Z Liu, M Qin, H Qian, Y Shu, Z Dou, JR Wen  
> Tackles query-less long-video understanding with a memorizing-reasoning-retrieving-focusing pipeline inspired by human memory.

---

### - **MomentSeeker: A Comprehensive Benchmark for Long Video Moment Retrieval**  | [Paper](https://arxiv.org/abs/2502.12558) | *Preprint*
**H Yuan**, J Ni, Y Wang, J Zhou, Z Liang, Z Liu, Z Cao, Z Dou, JR Wen  
> Introduces a benchmark with 500s+ videos and diverse tasks; includes an MLLM retriever fine-tuned on synthetic data.

---

### - **OmniGen: Unified Image Generation**  | [Paper](https://arxiv.org/abs/2409.11340) | *CVPR 2025*  
S Xiao*, Y Wang*, J Zhou*, **H Yuan***, X Xing, R Yan, S Wang, T Huang, Z Liu  
> A unified diffusion model that handles text-to-image, image editing, and conditional generation via a simple E2E architecture.

---

### - **FineRAG: Fine-grained Retrieval-Augmented Text-to-Image Generation**  | [Paper](https://aclanthology.org/2025.coling-main.741/) | *COLING 2024*  
**H Yuan**, Z Zhao, S Wang, S Xiao, M Ni, Z Liu, Z Dou  
> Breaks the RAG pipeline into 4 stages: query decomposition, candidate selection, retrieval-augmented diffusion, and self-reflection.

---

### - **VILE: Block-Aware Visual Enhanced Document Retrieval** | [Paper](https://dl.acm.org/doi/10.1145/3583780.3615107) | *CIKM 2023*  
**H Yuan**, Z Dou, Y Zhou, Y Guo, JR Wen  
> Proposes a dense retrieval model that fuses visual and textual signals to improve web page understanding.

---

# ğŸ’¼ Experiences

- *2024.12 â€“ Present*, **Research Intern**, BAAI  
  Supervised by [Zheng Liu](https://zhengliu101.github.io/)

- *2024.02 â€“ 2024.04*, **Research Intern**, Microsoft Research Asia  
  Supervised by [Chenfei Wu](https://chenfei-wu.github.io/) & [Nan Duan](https://nanduan.github.io/)  

---

# ğŸ† Competition

- ğŸ¥ˆ **ICPC Asia Shenyang** â€“ Silver Medal  
- ğŸ¥‰ **ICPC Asia Kunming** â€“ Bronze Medal  
- ğŸ¥ˆ **Mathematical Modeling Contest** â€“ National Second Prize

---

# ğŸ– Scholarships

- ğŸ¥‡ First-Class Scholarship, Gaoling School of AI â€“ *2022.12*  
- ğŸ¥‡ First-Class Scholarship (Top 5%), Nankai University â€“ *2021.12*, *2020.12*  
- ğŸ… **National Scholarship** (Top 1.2%), Nankai University â€“ *2019.12*
